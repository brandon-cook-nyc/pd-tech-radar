name,ring,quadrant,isNew,description
Decoupling Deployments from Release,Adopt,Techniques,TRUE,"<p>Test Implementing <a href=""http://martinfowler.com/bliki/ContinuousDelivery.html"" target=""_blank"">Continuous Delivery</a> continues to be a challenge for many organizations, and it remains important to highlight useful techniques such as <strong>decoupling deployment from release</strong>. We recommend strictly using the term <em>Deployment</em> when referring to the act of deploying a change to application components or infrastructure. The term <em>Release</em> should be used when a feature change is released to end users, with a business impact. Using techniques such as feature toggles and dark launches, we can deploy changes to production systems more frequently without releasing features. More-frequent deployments reduce the risk associated with change, while business stakeholders retain control over when features are released to end users.</p>"
TDD,Adopt,Techniques,FALSE,<p>Test Driven Development is a technique that has been valuable for us to ensure that all of our code is self tested. This enables teams to make changes and refactor with confidence without needing to perform a significant amount of manual or regression testing.</p>
Pragmatic Remote Pairing,Adopt,Techniques,FALSE,"<p>We firmly believe that <a href=""https://martinfowler.com/articles/on-pair-programming.html"" target=""_blank"">pair programming</a> improves the quality of code, spreads knowledge throughout a team and allows overall faster delivery of software. In a post COVID-19 world, however, many software teams will be distributed or fully remote, and in this situation we recommend <strong>pragmatic remote pairing</strong>: adjusting pairing practices to what's possible given the tools at hand. Consider tools such as <a href=""https://www.thoughtworks.com/radar/tools/visual-studio-live-share"">Visual Studio Live Share</a> for efficient, low-latency collaboration. Only resort to pixel-sharing if both participants reside in relative geographic proximity and have high-bandwidth internet connections. Pair developers who are in similar time zones rather than expecting pairing to work between participants regardless of their location. If pairing isn't working for logistical reasons, fall back to practices such as individual programming augmented via code reviews, pull-request collaboration (but beware <a href=""https://www.thoughtworks.com/radar/techniques/long-lived-branches-with-gitflow"">long-lived branches with Gitflow</a>) or shorter pairing sessions for critical parts of the code. We've engaged in remote pairing for years, and we've found it to be effective if done with a dose of pragmatism.</p>"
Trunk Based Development,Adopt,Techniques,FALSE,"<p>Trunk-based development is a version control management practice where developers merge small, frequent updates to a core “trunk” or main branch. Since it streamlines merging and integration phases, it helps achieve CI/CD and increases software delivery and organizational performance.</p>"
Consumer-driven Contract testing ,Adopt,Techniques,TRUE,"<p>We’ve decided to bring <strong>consumer-driven contract testing</strong> back from the archive for this edition even though we had allowed it to fade in the past. The concept isn’t new, but with the mainstream acceptance of microservices, we need to remind people that <a href=""http://www.martinfowler.com/articles/consumerDrivenContracts.html"" target=""_blank"">consumer-driven contracts</a> are an essential part of a mature <a href=""http://martinfowler.com/articles/microservice-testing/"" target=""_blank"">microservice testing</a> portfolio, enabling independent service deployments. But in addition, we want to point out that consumer-driven contract testing is a technique and an attitude that requires no special tool to implement. We love frameworks like <a href=""https://github.com/realestate-com-au/pact"" target=""_blank"">Pact</a> because they make proper contract tests easier to implement in certain contexts. But we have noticed a tendency for teams to focus on the framework rather than on the general practice. Writing Pact tests is not a guarantee that you are creating consumer-driven contracts; likewise, in many situations you should be creating good consumer-driven contracts even where no pre-built testing tool exists.</p>"
Docker for builds,Adopt,Techniques,FALSE,"<p>One of the many innovative uses of <a href=""https://www.docker.com/"" target=""_blank"">Docker</a> that we’ve seen on our projects is a technique to manage build-time dependencies. In the past, it was common to run build agents on an OS, augmented with dependencies needed for the target build. But with Docker it is possible to run the compilation step in an isolated environment complete with dependencies without contaminating the build agent. This technique of using <strong>Docker for builds</strong>  has proven particularly useful for compiling Golang binaries, and the <a href=""https://github.com/CenturyLinkLabs/golang-builder"" target=""_blank"">golang-builder</a> container is available for this very purpose.</p>"
Pipelines for Infrastructure as Code,Adopt,Techniques,FALSE,"<p>The use of continuous delivery pipelines to orchestrate the release process for software has become a mainstream concept. CI/CD tools can be used to test server configuration (e.g., Chef cookbooks, Puppet modules, Ansible playbooks), server image building (e.g., <a href=""https://www.thoughtworks.com/radar/tools/packer"">Packer</a>), environment provisioning (e.g., <a href=""https://www.thoughtworks.com/radar/tools/terraform"">Terraform</a>, CloudFormation) and the integration of environments. The use of <strong>pipelines for infrastructure as code</strong> lets you find errors before changes are applied to operational environments — including environments used for development and testing. They also offer a way to ensure that infrastructure tooling is run consistently, using CI/CD agents rather than individual workstations. Our teams have had good results adopting this technique on their projects.</p>"
Ligthweight Architecture Decision Records,Adopt,Techniques,FALSE,"<p>Much documentation can be replaced with highly readable code and tests. In a world of <a href=""https://www.thoughtworks.com/radar/techniques/evolutionary-architecture"">evolutionary architecture</a>, however, it's important to record certain design decisions for the benefit of future team members as well as for external oversight. <strong>Lightweight Architecture Decision Records</strong> is a <a href=""http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions"" target=""_blank"">technique</a> for capturing important architectural decisions along with their context and consequences. We recommend <a href=""http://github.com/npryce/adr-tools"" target=""_blank"">storing these details in source control</a>, instead of a wiki or website, as then they can provide a record that remains in sync with the code itself. For most projects, we see no reason why you wouldn't want to use this technique. All teams should maintain this lightweight documentation within their services and elevate any cross cutting concerns at the portfolio level.</p>"
Performance Testing as a First Class Citizen,Adopt,Techniques,FALSE,"<p>While unit and acceptance testing are widely embraced as standard development practices, this trend has not continued into the realm of performance testing. Currently, the common tooling drives testers towards creating throw away code and a click-and-script mentality. Treating performance testing as a first-class citizen enables the creation of better tests that cover more functionality, leading to better tooling to create and run performance tests, resulting in a test suite that is maintainable and can itself be tested.</p>"
Micro frontends,Adopt,Techniques,TRUE,"<p>We've seen significant benefits from introducing <a href=""https://martinfowler.com/articles/microservices.html"" target=""_blank"">microservices</a>, which have allowed teams to scale the delivery of independently deployed and maintained services. Unfortunately, we've also seen many teams create a front-end monolith — a large, entangled browser application that sits on top of the back-end services — largely neutralizing the benefits of microservices. <strong>Micro frontends</strong> have continued to gain in popularity since they were first introduced. We've seen many teams adopt some form of this architecture as a way to manage the complexity of multiple developers and teams contributing to the same user experience. In June of last year, one of the originators of this technique published an <a href=""https://martinfowler.com/articles/micro-frontends.html"" target=""_blank"">introductory article</a> that serves as a reference for micro frontends. It shows how this style can be implemented using various web programming mechanisms and builds out an example application using <a href=""https://www.thoughtworks.com/radar/languages-and-frameworks/react-js"">React.js</a>. We're confident this style will grow in popularity as larger organizations try to decompose UI development across multiple teams.</p>"
Pipelines as Code,Adopt,Techniques,FALSE,"<p>The <strong>pipelines as code</strong> technique emphasizes that the configuration of delivery pipelines that build, test and deploy our applications or infrastructure should be treated as code; they should be placed under source control and modularized in reusable components with automated testing and deployment. As organizations move to decentralized autonomous teams building <a href=""https://martinfowler.com/articles/microservices.html"" target=""_blank"">microservices</a> or <a href=""https://www.thoughtworks.com/radar/techniques/micro-frontends"">micro frontends</a>, the need for engineering practices in managing pipelines as code increases to keep building and deploying software consistent within the organization. This need has given rise to delivery pipeline templates and tooling that enable a standardized way to build and deploy services and applications. Such tools use the <em>declarative delivery pipelines</em> of applications, adopting a pipeline blueprint to execute the underlying tasks for various stages of a delivery lifecycle such as build, test and deployment; and they abstract away implementation details. The ability to build, test and deploy pipelines as code should be one of the evaluation criteria for choosing a CI/CD tool.</p>"
Mutation Testing,Trial,Techniques,FALSE,<p>We are currently experimeting with mutation testing in order to essentially determine the quality of our tests. Mutation testing automates the process of changing code in order to determine if there is optimal test scenarios written. This goes beyond the basic code coverage metric and can be an addition to understand the quality of the test suite. They are quite intensive to run so we will need to continue to determine how we can run these tests on a periodic basis in our pipelines.</p>
Four key metrics,Trial,Techniques,TRUE,"<p>To measure software delivery performance, more and more organizations are defaulting to the <strong>four key metrics</strong> as defined by the <a href=""https://www.devops-research.com/"">DORA research</a> program: change lead time, deployment frequency, mean time to restore (MTTR) and change fail percentage. This research and its statistical analysis have shown a clear link between high-delivery performance and these metrics; they provide a great leading indicator for how a delivery organization as a whole is doing.</p>

<p>This currently in assess as we determine the best way to calculate these metrics without going overboard around tooling. We are starting off with rough tracking over high precision and automation</p>

<p>We recommend always to keep in mind the ultimate intention behind a measurement and use it to reflect and learn. For example, before spending weeks building up sophisticated dashboard tooling, consider just regularly taking the <a href=""https://www.devops-research.com/quickcheck.html"">DORA quick check</a> in team retrospectives. This gives the team the opportunity to reflect on which <a href=""https://www.devops-research.com/research.html#capabilities"">capabilities</a> they could work on to improve their metrics, which can be much more effective than overdetailed out-of-the-box tooling. Keep in mind that these four key metrics originated out of the organization-level research of high-performing teams, and the use of these metrics at a team level should be a way to reflect on their own behaviors, not just another set of metrics to add to the dashboard.</p>"
Service Mesh,Trial,Techniques,TRUE,"<p><strong>Service mesh</strong> is an approach to operating a secure, fast and reliable microservices ecosystem. It has been an important stepping stone in making it easier to adopt microservices at scale. It offers discovery, security, tracing, monitoring and failure handling. It provides these cross-functional capabilities without the need for a shared asset such as an API gateway or baking libraries into each service. A typical implementation involves lightweight reverse-proxy processes, aka sidecars, deployed alongside each service process in a separate container. Sidecars intercept the inbound and outbound traffic of each service and provide cross-functional capabilities mentioned above. This approach has relieved the distributed service teams from building and updating the capabilities that the mesh offers as code in their services. This has lead to an even easier adoption of <a href=""https://www.thoughtworks.com/radar/techniques/polyglot-programming"">polyglot programming</a> in a microservices ecosystem. Our teams have been successfully using this approach with open source projects such as <a href=""https://www.thoughtworks.com/radar/platforms/istio"">Istio</a> and we will continue to monitor other open service mesh implementations such as <a href=""http://linkerd.io/"" target=""_blank"">Linkerd</a> closely.</p>"
Data Vault 2.0,Trial,Techniques,TRUE,"<p>Data Vault is a method and architecture for delivering a Data Analytics Service to an enterprise supporting its Business Intelligence, Data Warehousing, Analytics and Data Science requirements.  At the core it is a modern, agile way of designing and building efficient, effective Data Warehouses.</p>"
Data Mesh,Trial,Techniques,TRUE,"<p><a href=""https://martinfowler.com/articles/data-monolith-to-mesh.html"" target=""_blank""><strong>Data mesh</strong></a> is a <em>decentralized</em> organizational and technical approach in sharing, accessing and managing data for analytics and ML. Its objective is to create a <em>sociotechnical</em> approach that scales out getting value from data as the organization's complexity grows and as the use cases for data proliferate and the sources of data diversify. Essentially, it creates a <em>responsible</em> data-sharing model that is in step with organizational growth and continuous change. In our experience, interest in the application of data mesh has grown tremendously. The approach has inspired many organizations to embrace its adoption and technology providers to repurpose their existing technologies for a mesh deployment. Despite the great interest and growing experience in data mesh, its implementations face high cost of integration. Moreover, its adoption remains limited to sections of larger organizations and technology vendors are distracting the organizations from the hard <em>socio</em> aspects of data mesh — decentralized data ownership and a federated governance operating model.</p>"
Node.js for Microservices,Assess,Techniques,TRUE,"<p>We are looking to start considering building some of our microservices with node.js due to some json intensive workloads. However, we should be wary of Node Overload and only pick the tool if we feel that it will perform and scale well.</p>"
Observability as Code,Assess,Techniques,TRUE,"<p>The observability is an integral part of operating a distributed and <a href=""https://martinfowler.com/articles/microservices.html"" target=""_blank"">microservices architecture</a>. We rely on different system outputs such as distributed tracing, aggregate logs and metrics to infer the internal state of the distributed components, diagnose where the problems are and get to the root cause. An important aspect of an observability ecosystem is monitoring—visualizing and analyzing the system's output—and alerting when unexpected conditions are detected. Traditionally, configuration of monitoring dashboards and setting up alerts is done through GUI-based point-and-click systems. This approach leads to nonrepeatable dashboard configurations, no ability to continuously test and adjust alerts to avoid alert fatigue or missing out on important alerts, and drift from organizational best practices. We highly recommend treating your observability ecosystem configurations as code, called <strong>observability as code</strong> , and adopt <a href=""https://www.thoughtworks.com/radar/tools/infrastructure-as-code"">infrastructure as code</a> for your monitoring and alerting infrastructure. Choose observability products that support configuration through version-controlled code and execution of APIs or commands via infrastructure CD pipelines. Observability as code is an often-forgotten aspect of infrastructure as code and, we believe, crucial enough to be called out.</p>"
Micro frontends for Mobile,Assess,Techniques,TRUE,"<p>Since introducing them in the Radar in 2016, we've seen widespread adoption of <a href=""https://www.thoughtworks.com/radar/techniques/micro-frontends"">micro frontends</a> for web UIs. Recently, however, we've seen projects extend this architectural style to include <strong>micro frontends for mobile</strong> applications as well. When the application becomes sufficiently large and complex, it becomes necessary to distribute the development over multiple teams. This presents the challenge of maintaining team autonomy while integrating their work into a single app. Some teams write their own frameworks to enable this development style, and in the past we've mentioned <a href=""https://www.thoughtworks.com/radar/languages-and-frameworks/atlas-and-beehive"">Atlas and Beehive</a> as possible ways to simplify the problem of integrating multiteam app development. More recently, we've seen teams using <a href=""https://www.thoughtworks.com/radar/languages-and-frameworks/react-native"">React Native</a> to accomplish the same thing. Each React Native micro frontend is kept in its own repository where it can be built, tested and deployed separately. The team responsible for the overall application can then aggregate those micro frontends built by different teams into a single released app.</p>"
Front-end integration via Artifact,Hold,Techniques,FALSE,"<p>When teams embrace the concept of <a href=""https://www.thoughtworks.com/radar/techniques/micro-frontends"">micro frontends</a> they have a number of patterns at their disposal to integrate the individual micro frontends into one application. As always there are antipatterns, too. A common one in this case is <strong>front-end integration via artifact</strong>. For each micro frontend an artifact is built, usually an NPM package, which is pushed into a registry. A later step, sometimes in a different build pipeline, then combines the individual packages into a final package that contains all micro frontends. From a purely technical perspective this integration at build time results in a working application. However, integrating via artifact implies that for each change the full artifact needs to be rebuilt, which is time consuming and will likely have a negative impact on developer experience. Worse, this style of integrating frontends also introduces direct dependencies between the micro frontends at build time and therefore causes considerable coordination overhead. We currently have one integration like this and it has resulted in a coupling of deployments between two teams adding for more organizational overhead.</p>"
Micro frontends Anarchy,Hold,Techniques,TRUE,"<p>Since we originally introduced the term in 2016, <a href=""https://www.thoughtworks.com/radar/techniques/micro-frontends"">micro frontends</a> have grown in popularity and achieved mainstream acceptance. But like any new technique with an easy-to-remember name, it has occasionally been misused and abused. Particularly concerning is the tendency to use this architecture as an excuse to mix a range of competing technologies, tools or frameworks in a single page, leading to <strong>micro frontend anarchy</strong>. A particularly egregious form of this syndrome is using multiple frontend frameworks — for example, <a href=""https://www.thoughtworks.com/radar/languages-and-frameworks/react-js"">React.js</a> and <a href=""https://www.thoughtworks.com/radar/languages-and-frameworks/angular"">Angular</a> — in the same ""single-page"" application. Although this might be technically possible, it is far from advisable when not part of a deliberate transition strategy. Other properties that should be consistent from team to team include the styling technique (e.g., <a href=""https://www.thoughtworks.com/radar/languages-and-frameworks/css-in-js"">CSS-in-JS</a> or <a href=""https://www.thoughtworks.com/radar/languages-and-frameworks/css-modules"">CSS modules</a>) and the means by which the individual components are integrated (e.g., iFrames or <a href=""https://www.thoughtworks.com/radar/platforms/web-components-standard"">web components</a>). Furthermore, organizations should decide whether to standardize on consistent approaches or to leave it up to their teams to decide on state management, data fetching, build tooling, analytics and a host of other choices in a micro frontend application.</p>"
GitHub Actions,Adopt,Platforms,TRUE,"<p><strong><a href=""https://docs.github.com/en/actions"">GitHub Actions</a></strong> has grown considerably last year. It has proven that it can take on more complex workflows and call other actions in composite actions among other things. It still has some shortcomings, though, such as its inability to re-trigger a single job of a workflow. Although the ecosystem in the <a href=""https://github.com/marketplace?type=actions"">GitHub Marketplace</a> has its obvious advantages, giving third-party GitHub Actions access to your build pipeline risks sharing secrets in insecure ways (we recommend following GitHub's advice on <a href=""https://docs.github.com/en/actions/security-guides/security-hardening-for-github-actions"">security hardening</a>). However, the convenience of creating your build workflow directly in GitHub next to your source code combined with the ability to run GitHub Actions locally using open-source tools such as <a href=""https://github.com/nektos/act"">act</a> is a compelling option that has facilitated setup and onboarding of our teams.</p>

<p>Github Actions is the Firm wide choice for CI/CD pipelines and all new applications and services will need to build their pipelines using Github Actions with GoCD being deprecated. We need to continue to involve how we leverage Github Actions so that teams can create the most seamless CI/CD pipelines for their needs.</p>"
Heap,Adopt,Platforms,FALSE,"<p>strong><a href=""https://heap.io/"" target=""_blank"">Pendo</a></strong> is the leading digital insights platform that combines quantitative and qualitative analytics for a 360 degree view of your customer journey to see what’s really happening. With Heap you can see, understand, and act on every single thing your users do. This is currently the Firm wide recommend client side analytics tool.<p>"
Pendo,Adopt,Platforms,FALSE,"<p><strong><a href=""https://www.pendo.io/"" target=""_blank"">Pendo</a></strong> is a great analytics and in-app guide platform that has enabled our teams to both track critical paths in our products as well as create helpful guides to help improve the experience and adoption. The ease of use allows for non-developers to create experiences in our applications saving team capacity. We did warn that when teams work with pendo that they have an agreed upon path to production workflow in order to ensure the guides and tooltips created are still tested similarly to other parts of development</p>"
Docker,Adopt,Platforms,FALSE,"<p>We remain excited about <a href=""https://www.docker.com/"" target=""_blank""><strong>Docker</strong></a> as it evolves from a tool to a complex platform of technologies. Development teams love Docker, as the Docker image format makes it easier to achieve parity between development and production, making for reliable deployments. It is a natural fit in a microservices-style application as a packaging mechanism for self-contained services. On the operational front, Docker support in monitoring tools (<a href=""https://www.thoughtworks.com/radar/tools/sensu"">Sensu</a>, <a href=""https://www.thoughtworks.com/radar/tools/prometheus"">Prometheus</a>, <a href=""https://github.com/google/cadvisor"" target=""_blank"">cAdvisor</a>, etc.), orchestration tools (<a href=""https://www.thoughtworks.com/radar/platforms/kubernetes"">Kubernetes</a>, <a href=""https://mesosphere.github.io/marathon/"" target=""_blank"">Marathon</a>, etc.) and deployment-automation tools reflect the growing maturity of the platform and its readiness for production use. A word of caution, though: There is a prevalent view of Docker and Linux containers in general as being ""lightweight virtualization,"" but we would not recommend using Docker as a secure process-isolation mechanism, though we are paying attention to the introduction of user namespaces and seccomp profiles in version 1.10 in this regard.</p>"
Kubernetes,Adopt,Platforms,FALSE,"<p>Since we last mentioned <strong>Kubernetes</strong> in the Radar, it has become the default solution for most of our clients when deploying containers into a cluster of machines. The alternatives didn’t capture as much mindshare, and in some cases our clients are even changing their ‘engine’ to Kubernetes. Kubernetes has become the container orchestration platform of choice for major public cloud platforms, including Microsoft's Azure Container Service and Google Cloud (see the <a href=""https://www.thoughtworks.com/radar/platforms/gke"">GKE</a> blip). And there are many useful products enriching the fast-growing Kubernetes ecosystem. <strong><a href=""https://aws.amazon.com/eks/"" target=""_blank"">AWS EKS</a></strong> is our new target destination for our applications as we continue to migrate for an on-prem kubernetes cluster to the cloud.</p>"
Kafka,Adopt,Platforms,FALSE,"<p>Many organizations are now looking closely at new data architectures that capture information as immutable sequences of events at scale. <a href=""http://kafka.apache.org/"" target=""_blank""><strong>Apache Kafka</strong></a> continues to build momentum as an open source messaging framework that provides a solution for publishing ordered event feeds to large numbers of independent, lightweight consumers. Configuring Kafka is nontrivial, but our teams are reporting positive experiences with the framework.</p>"
Amazon Aurora,Adopt,Platforms,TRUE,<p>Amazon Aurora is a relational database management system (RDBMS) built for the cloud with full MySQL and PostgreSQL compatibility. Aurora gives you the performance and availability of commercial-grade databases at one-tenth the cost.</p>
Snowflake,Trial,Platforms,TRUE,"<p>Since we last mentioned <strong><a href=""https://www.snowflake.com/"" target=""_blank"">Snowflake</a></strong> in the Radar, we've gained more experience with it as well as with <a href=""https://www.thoughtworks.com/radar/techniques/data-mesh"">data mesh</a> as an alternative to data warehouses and lakes. Snowflake continues to impress with features like time travel, zero-copy cloning, data sharing and its marketplace. We also haven't found anything we don't like about it, all of which has led to our consultants generally preferring it over the alternatives. Redshift is moving toward storage and compute separation, which has been a strong point of Snowflake, but even with Redshift Spectrum it isn't as convenient and flexible to use, partly because it is bound by its Postgres heritage (we do still like <a href=""https://www.thoughtworks.com/radar/platforms/postgresql-for-nosql"">Postgres</a>, by the way). Federated queries can be a reason to go with Redshift. When it comes to operations, Snowflake is much simpler to run. <a href=""https://www.thoughtworks.com/radar/platforms/bigquery"">BigQuery</a>, which is another alternative, is very easy to operate, but in a multicloud setup Snowflake is a better choice. We can also report that we've used Snowflake successfully with <a href=""https://www.thoughtworks.com/radar/platforms/google-cloud-platform"">GCP</a>, <a href=""https://www.thoughtworks.com/radar/platforms/aws"">AWS</a>, and <a href=""https://www.thoughtworks.com/radar/platforms/azure"">Azure</a>.</p>"
Debezium,Trial,Platforms,TRUE,"<p><strong><a href=""https://debezium.io/"" target=""_blank"">Debezium</a></strong> is a <a href=""https://en.wikipedia.org/wiki/Change_data_capture"" target=""_blank"">change data capture (CDC)</a> platform that can stream database changes onto <a href=""https://www.thoughtworks.com/radar/tools/apache-kafka"">Kafka</a> topics. CDC is a popular technique with multiple use cases, including replicating data to other databases, feeding analytics systems, extracting microservices from monoliths and invalidating caches. Debezium reacts to changes in the database's log files and has CDC connectors for multiple databases, including Postgres, MySQL, Oracle and MongoDB. We're using Debezium in many projects, and it has worked very well for us.</p>"
Hasura,Assess,Platforms,TRUE,"<p> We are currently assessing <strong><a href=""https://hasura.io/"" target=""_blank"">Hasura</a></strong> as a platform to accerlerate API development. It provides capabilities around creating GraphQL endpoints on top of data as well as provides authorization around that data.</p>"
GoCD,Hold,Platforms,TRUE,<p>The Firm is moving away from supporting GoCD as platform for CI/CD. We caution our teams from investing to heavily in any existing GoCD pipelines and any new pipelines we encourage using Github Actions.<p>
Splunk,Adopt,Tools,FALSE,"<p>Application logs are both a blessing and a curse. They are comforting to have when a production issue arises, but actually digging out the data we need usually requires cobbling together scripts written in tools such as AWK and sed. Splunk is an elegant solution that quickly analyzes many standard log file formats like IIS, Log4J and syslog, and is extensible to custom formats. It indexes files, statically or in real time, to generate canned or custom reports. If the raw log fields do not provide what you need, simply use a regular expression, either inline or to define a new field, to get the desired level of detail. Splunk’s full power is difficult to describe, so we recommend downloading and trying it.</p>"
SonarQube,Adopt,Tools,FALSE,"<p><strong><a href=""https://www.sonarqube.org//"" target=""_blank"">SonarQube</a></strong> is the Firm approved way of performing static code and security analysis on our applications and services. All teams need to include this in their pipelines. However, more importantly teams need to leverage this tool in order to stay informed around issues and technical debt and demonstrate that they are using this information to make deliberate decisions rather than purely trying to hit certain metrics."
Storybook,Adopt,Tools,FALSE,"<p>As more and more teams embrace <a href=""https://www.thoughtworks.com/radar/techniques/designops"">DesignOps</a>, practices and tooling in this space mature. <strong>UI dev environments</strong> provide a comprehensive environment for quickly iterating on UI components, focusing on collaboration between user experience designers and developers. We now have a few options in this space: <a href=""https://storybook.js.org/"" target=""_blank"">Storybook</a>, <a href=""https://react-styleguidist.js.org/"" target=""_blank"">React Styleguidist</a>, <a href=""https://github.com/c8r"" target=""_blank"">Compositor</a> and <a href=""https://mdxjs.com/"" target=""_blank"">MDX</a>. You can use these tools standalone in component library or design system development as well as embedded in a web application project. Many teams were able to decrease their UI feedback cycles and improve timing of UI work in preparation for development work, which has made using UI dev environments a reasonable default for us. Our teams have converged on using Storybook and have been able to demostrate its ability to speed up the UI development process</p>"
Cypress,Adopt,Tools,FALSE,"<p><strong><a href=""http://www.cypress.io/"" target=""_blank"">Cypress</a></strong> is still a favorite among our teams where developers manage end-to-end tests themselves, as part of a healthy <a href=""https://martinfowler.com/articles/practical-test-pyramid.html#End-to-endTests"" target=""_blank"">test pyramid</a>, of course. We decided to call it out again in this Radar because recent versions of Cypress have added <a href=""https://cypress.io/blog/2020/02/06/introducing-firefox-and-edge-support-in-cypress-4-0/"" target=""_blank"">support for Firefox</a>, and we strongly suggest testing on multiple browsers. The dominance of Chrome and Chromium-based browsers has led to a worrying trend of teams seemingly only testing with Chrome which can lead to <a href=""https://twitter.com/mike_conley/status/1245797292453609478"" target=""_blank"">nasty surprises</a>.</p>"
ESLint,Adopt,Tools,FALSE,"<p><strong><a href=""https://eslint.org/"" target=""_blank"">ESLint</a></strong> is being used as a standard in many of our projects. As a linting tool for JavaScript it has multiple rule sets, recommended rules and plugins in order to extend to frameworks or JavaScript flavors. We've seen it leveraged heavily to help teams create and enforce norms in their code by allowing for real-time analysis of code during development. It can be used to standardize coding practices by enforcing best practices and code styling, and identify vulnerabilities in your code. It does so by integrating well with most IDEs and giving live feedback while coding. It's styling rules in particular will automatically fix the linting errors, making the process seamless and effective without incurring additional development cost. Developers can quickly get up to speed with the rules thanks to the community documentation, which does a good job of explaining coding patterns.  As ESLint becomes more common and powerful, it has gained traction in the industry, and this is illustrated by the <a href=""https://www.thoughtworks.com/radar/languages-and-frameworks/typescript"">TypeScript</a> team's move to support and work with ESLint rather than investing in TSLint.</p>"
semantic-release,Adopt,Tools,FALSE,"p><strong><a href=""https://github.com/semantic-release/semantic-release"" target=""_blank"">sematic-release</a></strong> </p>"
Gatling,Adopt,Tools,FALSE,"<p>Gatling is a tool in the automated performance testing space. It is similar to Locust and is much lighter weight than the older options such as JMeter and Grinder. Built on Scala, the DSL provides extensive functionality out of the box including easily configured data feeds and response assertions. In cases where customization is needed, it is easy to drop into Scala to provide extensions. The default generation of numerous dynamic views of the data via Highcharts adds to its appeal. We have had good success leveraging Gatling to assess the current performance for our applications and consider it as the default tool for our performance testing going forward.</p>"
ArchUnit,Adopt,Tools,FALSE,"<p><a href=""http://www.archunit.org/"" target=""_blank""><strong>ArchUnit</strong></a> is a Java testing library for checking architecture characteristics such as package and class dependencies, annotation verification and even layer consistency. We like that it runs as unit tests within your existing test setup, even though it supports only Java-based architectures. The ArchUnit test suite can be incorporated into a CI environment or a deployment pipeline, making it easier to implement <a href=""https://www.thoughtworks.com/radar/techniques/architectural-fitness-function"">fitness functions</a> in an <a href=""http://evolutionaryarchitecture.com/"" target=""_blank"">evolutionary architecture</a> way.</p>"
Buildkit on Docker,Trial,Tools,TRUE,"<p>Docker Build enhancements for 18.09 release introduces a much-needed overhaul of the build architecture. By integrating <strong><a href=""https://docs.docker.com/engine/reference/builder/#buildkit target=""_blank"">Buildkit</a></strong>, users should see an improvement on performance, storage management, feature functionality, and security.Docker images created with BuildKit can be pushed to Docker Hub just like Docker images created with legacy build. The Dockerfile format that works on legacy build will also work with BuildKit builds. The new --secret command line option allows the user to pass secret information for building new images with a specified Dockerfile. We have also seen performance gains by using buildkit with docker. Once we establish stability we will consider moving to adopt.</p>
"
CodeceptJS,Trial,Tools,FALSE,"<p><strong><a href=""https://codecept.io/"" target=""_blank"">CodeceptJS</a></strong> is a e2e testing tool that allows for a BDD style test suite. As a less battle tested Cypress we currently put this in Trial until we have better understanding of its capabilities and stability. Ideally we would like to converge on a singular e2e testing framework for future projects.<p>"
Terraform,Trial,Tools,TRUE,"<p><strong><a href=""https://www.terraform.io/"" target=""_blank"">Terraform</a></strong>, is rapidly becoming a de facto choice for creating and managing cloud infrastructures by writing declarative definitions. The configuration of the servers instantiated by Terraform is usually left to Puppet, Chef or Ansible. We like Terraform because the syntax of its files is quite readable and because it supports a number of cloud providers while making no attempt to provide an artificial abstraction across those providers. The active community will add support for the latest features from most cloud providers. Following our first, more cautious, mention of Terraform almost two years ago, it has seen continued development and has evolved into a stable product with a good ecosystem that has proven its value in our projects. The issue with state file management can now be sidestepped by using what Terraform calls a ""remote state backend."" We've successfully used <a href=""https://aws.amazon.com/s3/"" target=""_blank"">AWS S3</a> for that purpose. Currently at the Firm terraform is not ready to be used in our AWS environment, but we will need to migrate from CloudFormation once it is available.</p>"
Dynatrace,Trial,Tools,TRUE,"<p>We have regularly been using New Relic hosted performance monitoring with our systems in development and production. However, the Firm looks to be moving in a different direction in respect to monitoring and observability tools and is considering moving to Dynatrace. We will need to understand the feature set and see if we will be able to have the same capabilitiies as with New Relic</p>"
Stryker,Trial,Tools,FALSE,"<p><strong><a href=""https://stryker-mutator.io/"" target=""_blank"">Stryker</a></strong> is a relatively new entry in the mutation testing space. Similar to <a href=""https://www.thoughtworks.com/radar/tools/pitest"">Pitest</a>, Stryker lets you evaluate the quality of your tests. We've been using it quite successfully in JavaScript projects, but it also supports C# and Scala projects. Stryker is very user friendly and highly customizable, and we've been able to increase code coverage as well as confidence in the applications we're delivering for our clients.</p>"
Pitest,Trial,Tools,TRUE,"<p>Traditional testing approaches focus on evaluating if our production code is doing what it's supposed to do. However, we could make mistakes in the testing code introducing incomplete or useless assertions that create a false sense of confidence. This is where mutation testing comes in; it assesses the quality of the tests themselves, finding corner cases that are hard to realize. Our teams have used <strong><a href=""http://pitest.org/"" target=""_blank"">Pitest</a></strong> for a while now, and we recommend its use in Java projects to measure the health of the test suite. In short, mutation testing introduces changes in the production code and executes the same tests a second time; if the tests are still green it means that the tests are not good and need to improve. When you’re using programming languages other than Java <a href=""https://www.thoughtworks.com/radar/tools/stryker"">Stryker</a> is a good choice in this space.</p>"
CloudFormation,Assess,Tools,TRUE,"<p><a href=""https://aws.amazon.com/cloudformation/"" target=""_blank"">AWS CloudFormation</a> is a proprietary declarative language to provision AWS infrastructure as code. Handwriting CloudFormation files is often a default approach to bootstrap AWS infrastructure automation. Although this might be a sensible way to start a small project, our teams, and the industry at large, have found that <strong>handwritten CloudFormation</strong> simply does not scale as the infrastructure grows. Noticeable pitfalls of handwritten CloudFormation files for large projects include poor readability, lack of imperative constructs, limited parameter definition and usage, and lack of type checking. Addressing these shortfalls has led to a rich ecosystem of both open-source and custom tooling. We find <a href=""https://www.thoughtworks.com/radar/tools/terraform"">Terraform</a> a sensible default that not only addresses shortfalls of CloudFormation but also has an active community to add the latest AWS features and fix bugs. Currently, we need to use handwritten Cloudformation, but once Terraform is available we will most likely move CloudFormation to Hold</p>"
Pact,Assess,Tools,TRUE,"<p>Consumer-Driven Contracts are a testing approach to help service interfaces evolve with confidence without unknowingly breaking consumers. <a href=""https://pact.io/"" target=""_blank""><b>Pact</b></a> is a new open-source tool which allows testing interactions between service providers and consumers in isolation against a contract.</p>"
Pactflow,Assess,Tools,FALSE,"<p>We've used <a href=""https://github.com/pact-foundation"">Pact</a> for contract testing long enough to see some of the complexity that comes with scale. Some of our teams have successfully used <strong><a href=""https://pactflow.io/"">Pactflow</a></strong> to reduce that friction. Pactflow runs both as software as a service and as an on-prem deployment with the same features as the SaaS offering, and it adds improved usability, security and auditing on top of the open-source Pact Broker offering. We've been pleased with our use so far and are happy to see continued effort to remove some of the overhead of managing contract testing at scale.</p>"
Nx,Assess,Tools,TRUE,"<p>Over the years we've debated several times whether to feature monorepos in the Radar. Each time we ended up concluding that the trade-offs introduced by monorepos require a nuanced discussion and the technique is ""too complex to blip."" Now we're seeing increased interest in monorepos in the JavaScript community, for example, for building applications composed of micro frontends, as discussed in this <a href=""https://semaphoreci.com/blog/monorepo-micro-frontends-jonathan-creamer"" target=""_blank"">podcast episode</a>. Whether this is a good idea depends a lot on your situation, and we certainly don't want to give a general recommendation. What we do want to comment on is the tooling. In our teams we see a shift away from <a href=""https://lerna.js.org/"" target=""_blank"">Lerna</a> and a strong preference to use <a href=""https://nx.dev/"" target=""_blank""><strong>Nx</strong></a> for managing JavaScript-based monorepos.</p>"
dbt,Assess,Tools,TRUE,"<p>Since we last wrote about <strong><a href=""https://www.getdbt.com/"" target=""_blank"">dbt</a></strong>, we've used it in a few projects and like what we've seen. For example, we like that dbt makes the transformation part of ELT pipelines more accessible to consumers of the data as opposed to just the data engineers building the pipelines. It does this while encouraging good engineering practices such as versioning, automated testing and deployment. SQL continues to be the lingua franca of the data world (including databases, warehouses, query engines, data lakes and analytical platforms) and most of these systems support it to some extent. This allows dbt to be used against these systems for transformations by just building adaptors. The number of native connectors has grown to include <a href=""https://www.thoughtworks.com/radar/platforms/snowflake"">Snowflake</a>, <a href=""https://www.thoughtworks.com/radar/platforms/bigquery"">BigQuery</a>, Redshift and Postgres, as has the range of <a href=""https://docs.getdbt.com/docs/available-adapters"" target=""_blank"">community plugins</a>. We see tools like dbt helping data platforms become more ""self service"" capable.</p>"
urql,Assess,Tools,TRUE,"<p><strong><a href=""https://formidable.com/open-source/urql/"" target=""_blank"">urql</a></strong> is a highly customizable and versatile GraphQL client for React, Svelte, Vue, or plain JavaScript, with which you add on features like normalized caching as you grow. We are currently assessing using it within our React and React Native applications.</p>"
New Relic,Hold,Tools,FALSE,<p>We have regularly been using New Relic hosted performance monitoring with our systems in development and production. The combination of fast setup and comprehensive reporting has proven extremely valuable in troubleshooting performance. The reason this is not currently in adopt is that the Firm looks to be moving in a different direction in respect to monitoring and observability tools and is considering moving to Dynatrace</p>
Spring Boot,Adopt,languages-and-frameworks,FALSE,"<p>A lot of work has gone into <a href=""http://projects.spring.io/spring-boot"" target=""_blank""><strong>Spring Boot</strong></a> to reduce complexity and dependencies, which largely alleviates our previous reservations. If you live in a Spring ecosystem and are moving to microservices, Spring Boot is now the obvious choice.</p>

<p>We have been running most of our backend services successfully using Spring Boot for several years now with little issues</p>"
React,Adopt,languages-and-frameworks,FALSE,"<p>In the avalanche of front-end JavaScript frameworks, <a href=""http://facebook.github.io/react/"" target=""_blank""><strong>React.js</strong></a> stands out due to its design around a reactive data flow. Allowing only one-way data binding greatly simplifies the rendering logic and avoids many of the issues that commonly plague applications written with other frameworks. We're seeing the benefits of React.js on a growing number of projects, large and small, while at the same time we continue to be concerned about the state and the future of other popular frameworks like <a href=""https://www.thoughtworks.com/radar/languages-and-frameworks/angularjs"">AngularJS</a>. This has led to React.js becoming our default choice for JavaScript frameworks.</p>"
React Hooks,Adopt,languages-and-frameworks,FALSE,"<p><strong><a href=""https://reactjs.org/docs/hooks-intro.html"" target=""_blank"">React Hooks</a></strong> have introduced a new approach to managing stateful logic; given React components have always been closer to functions than classes, Hooks have embraced this and brought state to the functions, instead of using classes to take function to the state with methods. Another staple of state management in React applications is <a href=""https://www.thoughtworks.com/radar/languages-and-frameworks/redux"">Redux</a>, and we've already noted that it has come under scrutiny, suggesting that sometimes the complexity of Redux isn't worth it and in such cases a simple approach using Hooks is preferable. Rolling this completely on your own can quickly become tricky; therefore we recommend considering a combination of <a href=""https://reactjs.org/docs/context.html"" target=""_blank"">React Context</a> and the useContext and useReducer hooks, along the lines explained in this <a href=""https://blog.logrocket.com/guide-to-react-usereducer-hook/"" target=""_blank"">blog post</a>.</p>"
Typescript,Adopt,languages-and-frameworks,FALSE,"<p><strong><a href=""https://www.typescriptlang.org/"" target=""_blank"">TypeScript</a></strong>, a statically typed language and superset of JavaScript, has become our sensible default. Large-scale projects benefit most from the type safety. Our developers favor its minimal configuration management, well-integrated IDE support and its ability to refactor code safely and gradually adopt types. With its <a href=""https://definitelytyped.org/"" target=""_blank"">good repository</a> of TypeScript-type definitions at hand, we benefit from all the rich JavaScript libraries while gaining type safety.</p>"
SurveyJS,Adopt,languages-and-frameworks,FALSE,"<p><strong><a href=""https://github.com/surveyjs/"" target=""_blank"">SurveyJS</a></strong> is an Open-source JavaScript libraries to build surveys / forms. This is an alternative to cloud services and custom solutions. We have had great success using surveyjs in creating custom forms in our applications and it has enabled us to have more customizability for our product implementations.</p>"
Scala,Adopt,languages-and-frameworks,FALSE,"<p>Scala is a large language that is popular because of its approachability for new developers. This banquet of features is a problem because many aspects of Scala, like implicit conversions and dynamics, can get you into trouble. To successfully use Scala, you need to research the language and have a very strong opinion on which parts are right for you, creating your own definition of Scala, the good parts. You can disable the parts you do not want using a system called feature flags. We have been using Scala to write our performance tests using Gatling.</p>"
Webpack 5 Module Federation,Trial,languages-and-frameworks,TRUE,"<p>The release of the <strong><a href=""https://webpack.js.org/concepts/module-federation/"" target=""_blank"">Webpack 5 Module Federation</a></strong> feature has been highly anticipated by developers of <a href=""https://www.thoughtworks.com/radar/techniques/micro-frontends"">micro frontend</a> architectures. The feature introduces a more standardized way to optimize how module dependencies and shared code are managed and loaded. Module federation allows for the specification of shared modules, which helps with the deduplication of dependencies across micro frontends by loading code used by multiple modules only once. It also lets you distinguish between local and remote modules, where the remote modules are not actually part of the build itself but loaded asynchronously. Compared to build-time dependencies like npm packages, this can significantly simplify the deployment of a module update with many downstream dependencies. Be aware, though, that this requires you to bundle all of your micro frontends with Webpack, as opposed to approaches such as <a href=""https://www.thoughtworks.com/radar/techniques/import-maps-for-micro-frontends"">import maps</a>, which might eventually become part of the W3C standard.</p>"
React Native,Assess,languages-and-frameworks,TRUE,"<p>We are seeing continued success with <a href=""https://facebook.github.io/react-native/"" target=""_blank""><strong>React Native</strong></a> for rapid cross-platform mobile development. Despite some churn as it undergoes continuing development, the advantages of trivial integration between native and nonnative code and views, the rapid development cycle (instant reload, chrome debugging, Flexbox layout) and general growth of the React style is winning us over. As with many frameworks, care needs to be taken to keep your code well structured, but diligent use of a tool like <a href=""https://www.thoughtworks.com/radar/languages-and-frameworks/redux"">Redux</a> really helps here.</p>"
NestJS,Assess,languages-and-frameworks,TRUE,"<p>The growth in popularity of Node.js and trends such as <a href=""https://www.thoughtworks.com/radar/platforms/node-overload"">Node overload</a> have led to the application of Node.js for developing business applications. We often see problems, such as scalability and maintainability, with large JavaScript-based applications. <strong><a href=""https://nestjs.com/"" target=""_blank"">NestJS</a></strong> is a <a href=""https://www.thoughtworks.com/radar/languages-and-frameworks/typescript"">TypeScript-first</a> framework that makes the development of Node.js applications safer and less error prone. NestJS is opinionated and comes with SOLID principles and an Angular-inspired architecture out of the box. When building Node.js microservices, NestJS is one of the frameworks that our teams commonly use to empower developers to create testable, scalable, loosely coupled and easily maintainable applications.</p>"
